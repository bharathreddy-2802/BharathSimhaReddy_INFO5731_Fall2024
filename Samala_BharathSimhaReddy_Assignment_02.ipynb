{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bharathreddy-2802/BharathSimhaReddy_INFO5731_Fall2024/blob/main/Samala_BharathSimhaReddy_Assignment_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 2**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: Tuesday, at 11:59 PM.\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n",
        "\n",
        "**Please check that the link you submitted can be opened and points to the correct assignment.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "# Question 1 (40 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "(3) Collect all the reviews of the top 1000 most popular software from G2 or Capterra.\n",
        "\n",
        "(4) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the Densho Digital Repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install beautifulsoup4 requests pandas libraries\n",
        "!pip install beautifulsoup4 requests pandas\n",
        "\n",
        "# Importing required libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Headers to mimic browser behavior\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'\n",
        "}\n",
        "\n",
        "# Function to fetch reviews from Amazon\n",
        "def fetch_amazon_reviews(url):\n",
        "    reviews = []\n",
        "    for page in range(1, 21):  #  (each page might contain 50 reviews)\n",
        "        print(f'Scraping page {page}...')\n",
        "        # Send request to the website\n",
        "        page_url = f\"{url}/ref=cm_cr_arp_d_paging_btm_next_{page}?pageNumber={page}\"\n",
        "        response = requests.get(page_url, headers=headers)\n",
        "\n",
        "        # Parse the page content\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        review_blocks = soup.find_all('div', {'data-hook': 'review'})\n",
        "\n",
        "        # Extract review data\n",
        "        for review in review_blocks:\n",
        "            review_dict = {}\n",
        "            review_dict['Title'] = review.find('a', {'data-hook': 'review-title'}).text.strip() if review.find('a', {'data-hook': 'review-title'}) else ''\n",
        "            review_dict['Rating'] = review.find('i', {'data-hook': 'review-star-rating'}).text.strip() if review.find('i', {'data-hook': 'review-star-rating'}) else ''\n",
        "            review_dict['Body'] = review.find('span', {'data-hook': 'review-body'}).text.strip() if review.find('span', {'data-hook': 'review-body'}) else ''\n",
        "            reviews.append(review_dict)\n",
        "\n",
        "        # To avoid overloading the server, introduce a delay\n",
        "        time.sleep(2)\n",
        "\n",
        "    return reviews\n",
        "\n",
        "# URL of the Amazon product reviews (you need to change this URL for your desired product)\n",
        "product_url = 'https://www.amazon.com/SteelSeries-Worlds-Fastest-Mechanical-Keyboard/dp/B0BF64DN6H/ref=sr_1_1?_encoding=UTF8&content-id=amzn1.sym.12129333-2117-4490-9c17-6d31baf0582a&dib=eyJ2IjoiMSJ9.7_LpxWwuBa0EKw4v976atp6dIOFAq713J7JClcweWmx_OhgfhzTvvor7hYb8UD6IiikSLWVB1nJaFYudZuVIsaTjTji7EKbBoun_R_7EFp7j6dMXfc_FNHH59cDdhmPH2EmyCzOF4Z6xNIgx__nhGn6-XHTNvCzqckMAX1K2t0cl8-dC1vehcjyCftbfZZf1d6rIDaSi1d_XQ-oaVW3CdT0fy1WA2EjxlA8CVpkCZ14.r9MNsEpXFU2xiw_GshRo7XQg7-kCyo7SQ953k0RdBo0&dib_tag=se&keywords=gaming%2Bkeyboard&pd_rd_r=bcd4794e-56bb-4167-9b76-35977428cd25&pd_rd_w=sGF3k&pd_rd_wg=wdvBy&pf_rd_p=12129333-2117-4490-9c17-6d31baf0582a&pf_rd_r=77PCVSZR1175JTBF4PFG&qid=1727818392&refinements=p_72%3A1248885011&rnid=1248883011&s=videogames&sr=1-1&th=1'\n",
        "\n",
        "# Scrape the reviews\n",
        "amazon_reviews = fetch_amazon_reviews(product_url)\n",
        "\n",
        "# Save to a CSV file\n",
        "df = pd.DataFrame(amazon_reviews)\n",
        "df.to_csv('keyboard_reviews.csv', index=False)\n",
        "print(\"Reviews successfully saved to keyboard_reviews.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-XR8hVH-UWK",
        "outputId": "76d2257f-c172-4499-8546-e710b27fd8e0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Scraping page 1...\n",
            "Scraping page 2...\n",
            "Scraping page 3...\n",
            "Scraping page 4...\n",
            "Scraping page 5...\n",
            "Scraping page 6...\n",
            "Scraping page 7...\n",
            "Scraping page 8...\n",
            "Scraping page 9...\n",
            "Scraping page 10...\n",
            "Scraping page 11...\n",
            "Scraping page 12...\n",
            "Scraping page 13...\n",
            "Scraping page 14...\n",
            "Scraping page 15...\n",
            "Scraping page 16...\n",
            "Scraping page 17...\n",
            "Scraping page 18...\n",
            "Scraping page 19...\n",
            "Scraping page 20...\n",
            "Reviews successfully saved to keyboard_reviews.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "# Question 2 (30 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5QX6bJjGWXY9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b395e0e-fef1-45cd-bd9b-964f78c10938"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data:\n",
            "                                               Title              Rating  \\\n",
            "0  5.0 out of 5 stars\\nEASILY THE BEST, IF NOT TH...  5.0 out of 5 stars   \n",
            "1  5.0 out of 5 stars\\nBest keyboard you will EVE...  5.0 out of 5 stars   \n",
            "2                        5.0 out of 5 stars\\nAmazing  5.0 out of 5 stars   \n",
            "3  5.0 out of 5 stars\\nThe magnetic keys feel rea...  5.0 out of 5 stars   \n",
            "4                                                NaN                 NaN   \n",
            "\n",
            "                                                Body  \n",
            "0  I've used most of the top rated keyboards befo...  \n",
            "1  I bought this keyboard about a year and a half...  \n",
            "2  Amazing keyboard and a better ergonomics for t...  \n",
            "3  Really nice keyboard,  I am usually a hardcore...  \n",
            "4  I’ve upgraded from an older Corsair K70 to the...  \n",
            "\n",
            "Data after removing special characters:\n",
            "                                                Body  \\\n",
            "0  I've used most of the top rated keyboards befo...   \n",
            "1  I bought this keyboard about a year and a half...   \n",
            "2  Amazing keyboard and a better ergonomics for t...   \n",
            "3  Really nice keyboard,  I am usually a hardcore...   \n",
            "4  I’ve upgraded from an older Corsair K70 to the...   \n",
            "\n",
            "                                        cleaned_text  \n",
            "0  Ive used most of the top rated keyboards befor...  \n",
            "1  I bought this keyboard about a year and a half...  \n",
            "2  Amazing keyboard and a better ergonomics for t...  \n",
            "3  Really nice keyboard  I am usually a hardcore ...  \n",
            "4  Ive upgraded from an older Corsair K70 to the ...  \n",
            "\n",
            "Data after removing numbers:\n",
            "                                                Body  \\\n",
            "0  I've used most of the top rated keyboards befo...   \n",
            "1  I bought this keyboard about a year and a half...   \n",
            "2  Amazing keyboard and a better ergonomics for t...   \n",
            "3  Really nice keyboard,  I am usually a hardcore...   \n",
            "4  I’ve upgraded from an older Corsair K70 to the...   \n",
            "\n",
            "                                        cleaned_text  \n",
            "0  Ive used most of the top rated keyboards befor...  \n",
            "1  I bought this keyboard about a year and a half...  \n",
            "2  Amazing keyboard and a better ergonomics for t...  \n",
            "3  Really nice keyboard  I am usually a hardcore ...  \n",
            "4  Ive upgraded from an older Corsair K to the  v...  \n",
            "\n",
            "Data after removing stopwords:\n",
            "                                                Body  \\\n",
            "0  I've used most of the top rated keyboards befo...   \n",
            "1  I bought this keyboard about a year and a half...   \n",
            "2  Amazing keyboard and a better ergonomics for t...   \n",
            "3  Really nice keyboard,  I am usually a hardcore...   \n",
            "4  I’ve upgraded from an older Corsair K70 to the...   \n",
            "\n",
            "                                        cleaned_text  \n",
            "0  Ive used top rated keyboards like Steelseries ...  \n",
            "1  bought keyboard year half ago best keyboard ev...  \n",
            "2  Amazing keyboard better ergonomics wrist keybo...  \n",
            "3  Really nice keyboard usually hardcore cherry k...  \n",
            "4  Ive upgraded older Corsair K version Apex TKL ...  \n",
            "\n",
            "Data after converting to lowercase:\n",
            "                                                Body  \\\n",
            "0  I've used most of the top rated keyboards befo...   \n",
            "1  I bought this keyboard about a year and a half...   \n",
            "2  Amazing keyboard and a better ergonomics for t...   \n",
            "3  Really nice keyboard,  I am usually a hardcore...   \n",
            "4  I’ve upgraded from an older Corsair K70 to the...   \n",
            "\n",
            "                                        cleaned_text  \n",
            "0  ive used top rated keyboards like steelseries ...  \n",
            "1  bought keyboard year half ago best keyboard ev...  \n",
            "2  amazing keyboard better ergonomics wrist keybo...  \n",
            "3  really nice keyboard usually hardcore cherry k...  \n",
            "4  ive upgraded older corsair k version apex tkl ...  \n",
            "\n",
            "Data after stemming:\n",
            "                                        cleaned_text  \\\n",
            "0  ive used top rated keyboards like steelseries ...   \n",
            "1  bought keyboard year half ago best keyboard ev...   \n",
            "2  amazing keyboard better ergonomics wrist keybo...   \n",
            "3  really nice keyboard usually hardcore cherry k...   \n",
            "4  ive upgraded older corsair k version apex tkl ...   \n",
            "\n",
            "                                        stemmed_text  \n",
            "0  ive use top rate keyboard like steelseri razer...  \n",
            "1  bought keyboard year half ago best keyboard ev...  \n",
            "2  amaz keyboard better ergonom wrist keyboard us...  \n",
            "3  realli nice keyboard usual hardcor cherri key ...  \n",
            "4  ive upgrad older corsair k version apex tkl wi...  \n",
            "\n",
            "Data after lemmatization:\n",
            "                                        cleaned_text  \\\n",
            "0  ive used top rated keyboards like steelseries ...   \n",
            "1  bought keyboard year half ago best keyboard ev...   \n",
            "2  amazing keyboard better ergonomics wrist keybo...   \n",
            "3  really nice keyboard usually hardcore cherry k...   \n",
            "4  ive upgraded older corsair k version apex tkl ...   \n",
            "\n",
            "                                     lemmatized_text  \n",
            "0  ive used top rated keyboard like steelseries r...  \n",
            "1  bought keyboard year half ago best keyboard ev...  \n",
            "2  amazing keyboard better ergonomics wrist keybo...  \n",
            "3  really nice keyboard usually hardcore cherry k...  \n",
            "4  ive upgraded older corsair k version apex tkl ...  \n",
            "\n",
            "Cleaned data saved to 'amazon_reviews_cleaned.csv'\n"
          ]
        }
      ],
      "source": [
        "# Write code for each of the sub parts with proper comments.\n",
        "# Install  required pandas nltk libraries\n",
        "!pip install pandas nltk\n",
        "\n",
        "# libraries\n",
        "import re\n",
        "import nltk\n",
        "import pandas as pd\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "#nltk data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load the CSV file with Amazon reviews\n",
        "df = pd.read_csv('keyboard_reviews.csv')  #\n",
        "\n",
        "# Displayingt few rows of the dataset\n",
        "print(\"Original Data:\")\n",
        "print(df.head())\n",
        "\n",
        "# Part 1: Remove noise (special characters and punctuations)\n",
        "def remove_special_characters(text):\n",
        "    return re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "df['cleaned_text'] = df['Body'].apply(remove_special_characters)\n",
        "print(\"\\nData after removing special characters:\")\n",
        "print(df[['Body', 'cleaned_text']].head())\n",
        "\n",
        "# Part 2: Remove numbers\n",
        "def remove_numbers(text):\n",
        "    return re.sub(r'\\d+', '', text)\n",
        "\n",
        "df['cleaned_text'] = df['cleaned_text'].apply(remove_numbers)\n",
        "print(\"\\nData after removing numbers:\")\n",
        "print(df[['Body', 'cleaned_text']].head())\n",
        "\n",
        "# Part 3: Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "def remove_stopwords(text):\n",
        "    words = word_tokenize(text)\n",
        "    return ' '.join([word for word in words if word.lower() not in stop_words])\n",
        "\n",
        "df['cleaned_text'] = df['cleaned_text'].apply(remove_stopwords)\n",
        "print(\"\\nData after removing stopwords:\")\n",
        "print(df[['Body', 'cleaned_text']].head())\n",
        "\n",
        "# Part 4: Convert to lowercase\n",
        "df['cleaned_text'] = df['cleaned_text'].str.lower()\n",
        "print(\"\\nData after converting to lowercase:\")\n",
        "print(df[['Body', 'cleaned_text']].head())\n",
        "\n",
        "# Part 5: Stemming\n",
        "stemmer = PorterStemmer()\n",
        "def stem_text(text):\n",
        "    words = word_tokenize(text)\n",
        "    return ' '.join([stemmer.stem(word) for word in words])\n",
        "\n",
        "df['stemmed_text'] = df['cleaned_text'].apply(stem_text)\n",
        "print(\"\\nData after stemming:\")\n",
        "print(df[['cleaned_text', 'stemmed_text']].head())\n",
        "\n",
        "# Part 6: Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemmatize_text(text):\n",
        "    words = word_tokenize(text)\n",
        "    return ' '.join([lemmatizer.lemmatize(word) for word in words])\n",
        "\n",
        "df['lemmatized_text'] = df['cleaned_text'].apply(lemmatize_text)\n",
        "print(\"\\nData after lemmatization:\")\n",
        "print(df[['cleaned_text', 'lemmatized_text']].head())\n",
        "\n",
        "# Save the cleaned data to a new CSV file\n",
        "df.to_csv('keyboard_reviews_cleaned.csv', index=False)\n",
        "print(\"\\nCleaned data saved to 'amazon_reviews_cleaned.csv'\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "# Question 3 (30 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Y0oOSlsOS0cq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4fa43c4-5b4f-4f35-b942-593f9ee37937"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Collecting benepar\n",
            "  Downloading benepar-0.2.0.tar.gz (33 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from benepar) (2.4.1+cu121)\n",
            "Collecting torch-struct>=0.5 (from benepar)\n",
            "  Downloading torch_struct-0.5-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: tokenizers>=0.9.4 in /usr/local/lib/python3.10/dist-packages (from benepar) (0.19.1)\n",
            "Requirement already satisfied: transformers>=4.2.2 in /usr/local/lib/python3.10/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (4.44.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from benepar) (3.20.3)\n",
            "Requirement already satisfied: sentencepiece>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from benepar) (0.2.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.9.4->benepar) (0.24.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->benepar) (3.16.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->benepar) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->benepar) (3.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->benepar) (2024.6.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.2.2->transformers[tokenizers,torch]>=4.2.2->benepar) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.2.2->transformers[tokenizers,torch]>=4.2.2->benepar) (0.4.5)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (0.34.2)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.8.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[tokenizers,torch]>=4.2.2->benepar) (5.9.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->benepar) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Downloading torch_struct-0.5-py3-none-any.whl (34 kB)\n",
            "Building wheels for collected packages: benepar\n",
            "  Building wheel for benepar (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for benepar: filename=benepar-0.2.0-py3-none-any.whl size=37626 sha256=50462af822712661f97ab22ae0dd6f5d7ea02eaea85a2fcb79ba78c5f4c84671\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/4d/c1/a5af726368d5dbaaaa0b2dd36ed39b9da8cec46279a49bd6db\n",
            "Successfully built benepar\n",
            "Installing collected packages: torch-struct, benepar\n",
            "Successfully installed benepar-0.2.0 torch-struct-0.5\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.8.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Counts:\n",
            "Nouns: 4324\n",
            "Verbs: 1878\n",
            "Adjectives: 2355\n",
            "Adverbs: 643\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package benepar_en3 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping models/benepar_en3.zip.\n",
            "/usr/local/lib/python3.10/dist-packages/benepar/parse_chart.py:169: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/distributions/distribution.py:55: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: ive used top rated keyboard like steelseries razer blackwidow v green brown switch model razer huntsman red optical three well respected keyboard get lot praise built offerive played fps game year tend notice look new tech product give advantage gameplay besides im also fulltime engineering student find typing report frequently switch linear dont tactile click bump instead smooth keystroke perfect fps shooter allows faster better feeling key pressesi tested keyboard favorite shooter game main one play apex legend game like valorant aim heavy apex movement heavy game using keyboard mouse time lowest actuation setting mm nice quick movement pulling healsshield wheel throwable wheel apex legendsthe oled screen really cool feature actually allows change setting without steelseries engine application setting like brightness actuation macro plus custom message main screen display ive seen even connects game display killsdeathsassists game like csgoin term report actuation setting might responsive perosnally dont tend mistype much really help overall build really good wish steelseries used braided cable huge deal breaker ive got say steelseries made keyboard suit school gaming needsnot attractive keyboard always get complement asked keyboard every time someone see setup also replace standard key cap white one hk gamingso worth getting apex pro personally yesthe price bit high people agree get compared keyboard adjustable actuation point oled screen volume dial make top pick keyboardssteelseries done great job making attractive nice looking keyboard already fullsize apex pro needed second one second setup came buying second keyboard looking every option availablei usually tend want get something new different couldnt seem find keyboard looked sounded better apex pro personally overall keyboard good fun use im sure edge buying fear liking give time youll appreciate keyboard lotthis isnt keyboard get used keyboard grows hope help read\n",
            "\n",
            "Constituency Parsing Tree:\n",
            "(S (LS i) (PRP ve) (VP (VBD used) (NP (NP (ADJP (JJ top) (JJ rated)) (NN keyboard)) (PP (IN like) (NP (NN steelseries) (NN razer))) (NN blackwidow) (RB v) (NP (JJ green) (JJ brown)) (NN switch) (NN model) (NN razer) (NN huntsman) (JJ red) (JJ optical) (CD three) (ADJP (RB well) (VBN respected)) (NN keyboard)) (VB get) (NP (NP (NN lot) (NN praise)) (VBN built) (JJ offerive) (VBN played) (NP (FW fps) (NN game)) (NN year)) (VP (VBP tend) (PRT (VB notice)) (S (VB look) (NP (JJ new) (NN tech) (NN product)) (VB give) (NP (NP (NP (NN advantage)) (NN gameplay)) (IN besides) (VBP i) (VBP m) (ADVP (RB also)) (NP (NP (NP (JJ fulltime) (NN engineering) (NN student)) (VB find) (NP (VBG typing) (NN report))) (ADVP (RB frequently)) (VB switch) (JJ linear) (RB do) (RB nt) (NP (NP (JJ tactile) (NN click) (NN bump)) (RB instead) (NP (NP (NP (JJ smooth) (NN keystroke)) (JJ perfect) (NNS fps) (NN shooter)) (VP (VBZ allows) (NP (JJR faster) (RBR better) (NN feeling) (NP (NP (NN key) (`` pressesi)) (VBN tested) (NP (NN keyboard) (JJ favorite) (NN shooter) (NN game) (JJ main) (CD one) (S (VP (VB play) (NP (NP (NN apex) (NN legend) (NN game)) (PP (IN like) (NP (NP (NP (. valorant) (. aim) (JJ heavy) (FW apex) (NN movement)) (JJ heavy) (NN game)) (S (VP (VBG using) (NP (NP (NN keyboard) (NN mouse)) (NP (ADVP (NN time)) (NP (JJS lowest) (NN actuation) (NN setting) (NN mm) (NP (JJ nice) (JJ quick) (NN movement) (S (VP (VBG pulling) (NP (ADJP (NN healsshield) (NN wheel) (ADJP (JJ throwable) (NN wheel) (ADJP (NN apex) (DT legendsthe) (VBN oled) (NN screen) (ADJP (RB really) (JJ cool))))) (NN feature) (ADVP (RB actually)) (VBP allows) (NP (NP (VB change) (NN setting)) (PP (IN without) (NP (NP (NP (NP (NP (NP (NP (NP (NP (NP (NP (NP (NP (NP (NP (NP (NP (NP (NP (NP (NP (NP (NP (NP (NP (NP (NP (NP (NP (NP (NP (NP (NP (NP (NP (NP (NP (NP (NP (NP (NP (NNS steelseries)) (NN engine) (NN application)) (NN setting)) (PP (IN like) (NP (NP (NN brightness) (NN actuation) (NN macro)) (CC plus) (NP (NP (NP (JJ custom) (NN message)) (NML (JJ main) (NN screen))) (NN display))))) (RB i) (RB ve) (VBN seen)) (ADVP (RB even)) (VBZ connects) (NP (NN game) (NN display))) (NNS killsdeathsassists) (NN game)) (IN like) (NP (NN csgoin))) (NN term) (NN report)) (NN actuation) (NN setting)) (MD might) (ADJP (JJ responsive))) (ADVP (RB perosnally)) (RB do) (RB nt) (VB tend) (S (VP (NN mistype)))) (ADVP (RB much) (ADVP (RB really)) (VB help) (ADVP (RB overall)))) (VB build)) (ADVP (ADJP (RB really) (JJ good)) (VB wish))) (NP (NNS steelseries))) (VBN used)) (S (VP (VBN braided) (NN cable)))) (NP (NP (JJ huge) (NN deal)) (NP (NN breaker)))) (. i) (. ve) (VBD got)) (VB say) (NP (NNS steelseries))) (VBN made) (NP (NN keyboard))) (NN suit)) (NP (NN school) (NN gaming))) (RB needsnot) (JJ attractive) (NN keyboard)) (ADVP (ADVP (RB always)) (VB get) (NP (NP (ADJP (NN complement) (VBN asked)) (NN keyboard)) (NP (DT every) (NN time))))) (NP (NN someone))) (VP (VB see) (NP (NN setup)))) (ADVP (RB also)) (VB replace) (NP (JJ standard) (NN key) (NN cap))) (JJ white) (CD one) (NN hk)) (RB gamingso)) (JJ worth)) (S (VP (VBG getting) (NP (NN apex) (NN pro))))) (ADVP (RB personally))) (NP (DT yesthe) (NN price))) (RB bit)) (ADJP (JJ high))) (NNS people) (VBP agree) (VB get) (VBN compared) (NN keyboard)) (JJ adjustable) (NN actuation) (NN point)) (VBN oled) (NN screen)) (NP (NN volume) (NN dial)))))))))))))))))))))))))))) (VP (VB make) (S (NP (ADJP (NP (NP (NP (NP (NML (JJ top) (NN pick)) (NNS keyboardssteelseries)) (VBN done) (JJ great) (NN job)) (VBG making) (JJ attractive) (JJ nice) (JJ looking) (NN keyboard)) (ADVP (RB already)) (JJ fullsize) (NN apex) (FW pro)) (ADJP (VBN needed) (ADVP (JJ second) (CD one) (JJ second)))) (NN setup)) (VP (VBD came) (S (VP (S (VP (VBG buying) (JJ second) (NN keyboard) (VBG looking) (NP (DT every) (NN option)))) (PRP availablei) (ADVP (RB usually)) (VP (VBP tend) (S (VP (VB want) (S (VP (VB get) (NP (NP (NP (NP (NP (NP (NP (NP (NP (NP (NP (NP (NP (NP (NN something)) (ADJP (JJ new) (JJ different))) (MD could) (VB nt) (VB seem)) (VB find) (NP (NN keyboard))) (VBD looked)) (VBN sounded)) (RBR better)) (NP (NN apex) (FW pro))) (ADVP (RB personally))) (JJ overall) (NN keyboard)) (JJ good) (JJ fun) (NN use)) (. i) (. m) (RB sure) (NN edge) (VBG buying) (NN fear) (VBG liking)) (VB give) (NP (NN time) (NP (PRP you) (MD ll) (VB appreciate) (NP (NP (NP (NN keyboard)) (DT lotthis) (. is) (. nt) (NN keyboard)) (VB get) (VBN used) (NP (NN keyboard)))))) (VP (VBZ grows) (NP (NN hope) (. help) (S (VP (VB read)))))))))))))))))))\n",
            "\n",
            "Dependency Parsing:\n",
            "i: nsubj --> used\n",
            "ve: aux --> used\n",
            "used: ROOT --> used\n",
            "top: amod --> keyboard\n",
            "rated: amod --> keyboard\n",
            "keyboard: dobj --> used\n",
            "like: prep --> used\n",
            "steelseries: compound --> blackwidow\n",
            "razer: compound --> blackwidow\n",
            "blackwidow: pobj --> like\n",
            "v: prep --> blackwidow\n",
            "green: amod --> model\n",
            "brown: amod --> model\n",
            "switch: compound --> model\n",
            "model: compound --> huntsman\n",
            "razer: compound --> huntsman\n",
            "huntsman: compound --> keyboard\n",
            "red: nmod --> keyboard\n",
            "optical: amod --> keyboard\n",
            "three: nummod --> well\n",
            "well: npadvmod --> respected\n",
            "respected: amod --> keyboard\n",
            "keyboard: pobj --> v\n",
            "get: xcomp --> used\n",
            "lot: compound --> praise\n",
            "praise: nsubj --> built\n",
            "built: ccomp --> get\n",
            "offerive: amod --> year\n",
            "played: amod --> fps\n",
            "fps: compound --> game\n",
            "game: compound --> year\n",
            "year: npadvmod --> built\n",
            "tend: xcomp --> used\n",
            "notice: nsubj --> look\n",
            "look: ccomp --> tend\n",
            "new: amod --> product\n",
            "tech: compound --> product\n",
            "product: nsubj --> give\n",
            "give: ccomp --> look\n",
            "advantage: compound --> gameplay\n",
            "gameplay: dobj --> give\n",
            "besides: prep --> give\n",
            "i: pobj --> besides\n",
            "m: acl --> gameplay\n",
            "also: advmod --> m\n",
            "fulltime: amod --> student\n",
            "engineering: compound --> student\n",
            "student: nsubj --> find\n",
            "find: ccomp --> give\n",
            "typing: amod --> report\n",
            "report: nsubj --> switch\n",
            "frequently: advmod --> switch\n",
            "switch: ccomp --> find\n",
            "linear: dobj --> switch\n",
            "do: aux --> tactile\n",
            "nt: neg --> tactile\n",
            "tactile: aux --> click\n",
            "click: advcl --> switch\n",
            "bump: dobj --> click\n",
            "instead: advmod --> click\n",
            "smooth: amod --> shooter\n",
            "keystroke: compound --> shooter\n",
            "perfect: compound --> shooter\n",
            "fps: compound --> shooter\n",
            "shooter: nsubj --> allows\n",
            "allows: conj --> used\n",
            "faster: advmod --> better\n",
            "better: advmod --> feeling\n",
            "feeling: xcomp --> allows\n",
            "key: amod --> pressesi\n",
            "pressesi: nsubj --> tested\n",
            "tested: ccomp --> feeling\n",
            "keyboard: compound --> favorite\n",
            "favorite: amod --> game\n",
            "shooter: amod --> game\n",
            "game: nmod --> game\n",
            "main: amod --> game\n",
            "one: nummod --> apex\n",
            "play: compound --> apex\n",
            "apex: compound --> game\n",
            "legend: compound --> game\n",
            "game: dobj --> tested\n",
            "like: prep --> game\n",
            "valorant: pobj --> like\n",
            "aim: nmod --> game\n",
            "heavy: compound --> apex\n",
            "apex: compound --> movement\n",
            "movement: nmod --> game\n",
            "heavy: amod --> game\n",
            "game: appos --> game\n",
            "using: acl --> game\n",
            "keyboard: compound --> mouse\n",
            "mouse: compound --> time\n",
            "time: nmod --> actuation\n",
            "lowest: amod --> actuation\n",
            "actuation: dobj --> using\n",
            "setting: acl --> actuation\n",
            "mm: nmod --> movement\n",
            "nice: amod --> movement\n",
            "quick: amod --> movement\n",
            "movement: dobj --> setting\n",
            "pulling: acl --> movement\n",
            "healsshield: compound --> wheel\n",
            "wheel: nmod --> apex\n",
            "throwable: compound --> wheel\n",
            "wheel: compound --> apex\n",
            "apex: dobj --> pulling\n",
            "legendsthe: nmod --> screen\n",
            "oled: amod --> screen\n",
            "screen: nmod --> feature\n",
            "really: advmod --> cool\n",
            "cool: amod --> feature\n",
            "feature: nsubj --> allows\n",
            "actually: advmod --> allows\n",
            "allows: conj --> used\n",
            "change: nsubj --> setting\n",
            "setting: acl --> change\n",
            "without: prep --> setting\n",
            "steelseries: compound --> engine\n",
            "engine: compound --> application\n",
            "application: pobj --> without\n",
            "setting: nsubj --> tend\n",
            "like: prep --> setting\n",
            "brightness: compound --> actuation\n",
            "actuation: compound --> macro\n",
            "macro: pobj --> like\n",
            "plus: cc --> macro\n",
            "custom: compound --> message\n",
            "message: nmod --> display\n",
            "main: amod --> screen\n",
            "screen: compound --> display\n",
            "display: conj --> macro\n",
            "i: nsubj --> seen\n",
            "ve: aux --> seen\n",
            "seen: relcl --> setting\n",
            "even: advmod --> connects\n",
            "connects: xcomp --> seen\n",
            "game: compound --> display\n",
            "display: nsubj --> killsdeathsassists\n",
            "killsdeathsassists: compound --> game\n",
            "game: nsubj --> responsive\n",
            "like: prep --> game\n",
            "csgoin: compound --> term\n",
            "term: compound --> report\n",
            "report: compound --> actuation\n",
            "actuation: compound --> setting\n",
            "setting: pcomp --> like\n",
            "might: aux --> responsive\n",
            "responsive: ccomp --> connects\n",
            "perosnally: advmod --> tend\n",
            "do: aux --> tend\n",
            "nt: neg --> tend\n",
            "tend: ccomp --> allows\n",
            "mistype: dobj --> tend\n",
            "much: nsubj --> help\n",
            "really: advmod --> help\n",
            "help: ccomp --> tend\n",
            "overall: advmod --> build\n",
            "build: xcomp --> help\n",
            "really: advmod --> good\n",
            "good: amod --> steelseries\n",
            "wish: compound --> steelseries\n",
            "steelseries: dobj --> build\n",
            "used: acl --> steelseries\n",
            "braided: amod --> breaker\n",
            "cable: nmod --> breaker\n",
            "huge: amod --> breaker\n",
            "deal: compound --> breaker\n",
            "breaker: dobj --> used\n",
            "i: nsubj --> got\n",
            "ve: aux --> got\n",
            "got: aux --> say\n",
            "say: ccomp --> tend\n",
            "steelseries: nsubj --> made\n",
            "made: ccomp --> say\n",
            "keyboard: compound --> suit\n",
            "suit: compound --> school\n",
            "school: nsubj --> gaming\n",
            "gaming: nmod --> keyboard\n",
            "needsnot: nmod --> keyboard\n",
            "attractive: amod --> keyboard\n",
            "keyboard: nsubj --> get\n",
            "always: advmod --> get\n",
            "get: ccomp --> made\n",
            "complement: nsubj --> asked\n",
            "asked: ccomp --> get\n",
            "keyboard: dobj --> asked\n",
            "every: det --> time\n",
            "time: nsubj --> replace\n",
            "someone: nsubj --> see\n",
            "see: relcl --> time\n",
            "setup: dobj --> see\n",
            "also: advmod --> replace\n",
            "replace: ccomp --> get\n",
            "standard: amod --> cap\n",
            "key: amod --> cap\n",
            "cap: dobj --> replace\n",
            "white: advmod --> replace\n",
            "one: nummod --> gamingso\n",
            "hk: compound --> gamingso\n",
            "gamingso: dobj --> replace\n",
            "worth: amod --> gamingso\n",
            "getting: xcomp --> worth\n",
            "apex: nsubj --> pro\n",
            "pro: ccomp --> getting\n",
            "personally: advmod --> pro\n",
            "yesthe: det --> bit\n",
            "price: compound --> bit\n",
            "bit: npadvmod --> high\n",
            "high: amod --> people\n",
            "people: nsubj --> agree\n",
            "agree: relcl --> gamingso\n",
            "get: xcomp --> agree\n",
            "compared: amod --> dial\n",
            "keyboard: nmod --> point\n",
            "adjustable: amod --> actuation\n",
            "actuation: compound --> point\n",
            "point: npadvmod --> oled\n",
            "oled: amod --> dial\n",
            "screen: compound --> volume\n",
            "volume: compound --> dial\n",
            "dial: nsubj --> make\n",
            "make: conj --> replace\n",
            "top: amod --> keyboardssteelseries\n",
            "pick: compound --> keyboardssteelseries\n",
            "keyboardssteelseries: nsubj --> done\n",
            "done: ccomp --> make\n",
            "great: amod --> job\n",
            "job: dobj --> done\n",
            "making: acl --> job\n",
            "attractive: amod --> keyboard\n",
            "nice: amod --> keyboard\n",
            "looking: compound --> keyboard\n",
            "keyboard: nsubj --> fullsize\n",
            "already: advmod --> fullsize\n",
            "fullsize: ccomp --> make\n",
            "apex: nsubj --> needed\n",
            "pro: advmod --> needed\n",
            "needed: ccomp --> fullsize\n",
            "second: amod --> setup\n",
            "one: nummod --> setup\n",
            "second: amod --> setup\n",
            "setup: nsubj --> buying\n",
            "came: aux --> buying\n",
            "buying: advcl --> needed\n",
            "second: amod --> keyboard\n",
            "keyboard: dobj --> buying\n",
            "looking: advcl --> buying\n",
            "every: det --> availablei\n",
            "option: compound --> availablei\n",
            "availablei: nsubj --> tend\n",
            "usually: advmod --> tend\n",
            "tend: ccomp --> make\n",
            "want: xcomp --> tend\n",
            "get: xcomp --> want\n",
            "something: dobj --> get\n",
            "new: advmod --> different\n",
            "different: amod --> something\n",
            "could: aux --> seem\n",
            "nt: neg --> seem\n",
            "seem: ccomp --> make\n",
            "find: xcomp --> seem\n",
            "keyboard: nsubjpass --> sounded\n",
            "looked: auxpass --> sounded\n",
            "sounded: ccomp --> find\n",
            "better: amod --> apex\n",
            "apex: compound --> use\n",
            "pro: amod --> use\n",
            "personally: advmod --> pro\n",
            "overall: amod --> use\n",
            "keyboard: nmod --> use\n",
            "good: amod --> use\n",
            "fun: compound --> use\n",
            "use: dobj --> sounded\n",
            "i: npadvmod --> used\n",
            "m: ccomp --> used\n",
            "sure: advmod --> edge\n",
            "edge: xcomp --> used\n",
            "buying: xcomp --> edge\n",
            "fear: dobj --> buying\n",
            "liking: acl --> fear\n",
            "give: conj --> edge\n",
            "time: dobj --> give\n",
            "you: nsubj --> appreciate\n",
            "ll: aux --> appreciate\n",
            "appreciate: ccomp --> used\n",
            "keyboard: compound --> lotthis\n",
            "lotthis: nsubj --> is\n",
            "is: ccomp --> used\n",
            "nt: neg --> is\n",
            "keyboard: npadvmod --> get\n",
            "get: auxpass --> used\n",
            "used: acomp --> is\n",
            "keyboard: compound --> grows\n",
            "grows: xcomp --> used\n",
            "hope: compound --> help\n",
            "help: nsubj --> read\n",
            "read: xcomp --> used\n",
            "\n",
            "Named Entity Recognition (NER) Counts:\n",
            "NORP: 13\n",
            "DATE: 24\n",
            "CARDINAL: 37\n",
            "PERSON: 73\n",
            "ORDINAL: 48\n",
            "ORG: 97\n",
            "TIME: 12\n",
            "GPE: 12\n",
            "\n",
            "Sample NER Output:\n",
            "                                     lemmatized_text  \\\n",
            "0  ive used top rated keyboard like steelseries r...   \n",
            "1  bought keyboard year half ago best keyboard ev...   \n",
            "2  amazing keyboard better ergonomics wrist keybo...   \n",
            "3  really nice keyboard usually hardcore cherry k...   \n",
            "4  ive upgraded older corsair k version apex tkl ...   \n",
            "\n",
            "                                            entities  \n",
            "0  [(razer, NORP), (year, DATE), (one, CARDINAL),...  \n",
            "1  [(year half ago, DATE), (rgb, ORG), (rgb, ORG)...  \n",
            "2     [(amazing keyboard better, ORG), (hour, TIME)]  \n",
            "3                                                 []  \n",
            "4                                                 []  \n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "# Install nltk spacy benepar libraries\n",
        "!pip install nltk spacy benepar\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "# Importing required libraries\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import spacy\n",
        "from collections import Counter\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from spacy import displacy\n",
        "import benepar\n",
        "\n",
        "# Load the cleaned dataset\n",
        "df = pd.read_csv('keyboard_reviews_cleaned.csv')  # Adjust the path to the cleaned CSV if needed\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Part 1: POS Tagging (Parts of Speech)\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Function to perform POS tagging\n",
        "def pos_tagging(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    return pos_tag(tokens)\n",
        "\n",
        "# Apply POS tagging to the cleaned text\n",
        "df['pos_tags'] = df['lemmatized_text'].apply(pos_tagging)\n",
        "\n",
        "# Counting Nouns, Verbs, Adjectives, and Adverbs\n",
        "pos_counts = Counter()\n",
        "\n",
        "for tags in df['pos_tags']:\n",
        "    for word, pos in tags:\n",
        "        if pos.startswith('N'):\n",
        "            pos_counts['Noun'] += 1\n",
        "        elif pos.startswith('V'):\n",
        "            pos_counts['Verb'] += 1\n",
        "        elif pos.startswith('J'):\n",
        "            pos_counts['Adjective'] += 1\n",
        "        elif pos.startswith('R'):\n",
        "            pos_counts['Adverb'] += 1\n",
        "\n",
        "print(f\"POS Counts:\\nNouns: {pos_counts['Noun']}\\nVerbs: {pos_counts['Verb']}\\nAdjectives: {pos_counts['Adjective']}\\nAdverbs: {pos_counts['Adverb']}\")\n",
        "\n",
        "# Part 2: Constituency Parsing and Dependency Parsing\n",
        "# Benepar parser for constituency parsing\n",
        "benepar.download('benepar_en3')\n",
        "nlp.add_pipe(\"benepar\", config={\"model\": \"benepar_en3\"})\n",
        "\n",
        "# Function to print constituency and dependency parsing for a sentence\n",
        "def parse_sentence(text):\n",
        "    doc = nlp(text)\n",
        "    for sent in doc.sents:\n",
        "        print(f\"Sentence: {sent.text}\")\n",
        "        print(\"\\nConstituency Parsing Tree:\")\n",
        "        print(sent._.parse_string)\n",
        "        print(\"\\nDependency Parsing:\")\n",
        "        for token in sent:\n",
        "            print(f\"{token.text}: {token.dep_} --> {token.head.text}\")\n",
        "\n",
        "# Example sentence for parsing (choose one sentence for explanation)\n",
        "example_sentence = df['lemmatized_text'].iloc[0]  # First sentence from the cleaned text\n",
        "parse_sentence(example_sentence)\n",
        "\n",
        "# Part 3: Named Entity Recognition (NER)\n",
        "# Function to extract named entities\n",
        "def extract_entities(text):\n",
        "    doc = nlp(text)\n",
        "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "    return entities\n",
        "\n",
        "# Apply NER to the lemmatized text\n",
        "df['entities'] = df['lemmatized_text'].apply(extract_entities)\n",
        "\n",
        "# Count the number of each entity type\n",
        "entity_counts = Counter()\n",
        "\n",
        "for entities in df['entities']:\n",
        "    for text, label in entities:\n",
        "        entity_counts[label] += 1\n",
        "\n",
        "print(\"\\nNamed Entity Recognition (NER) Counts:\")\n",
        "for entity, count in entity_counts.items():\n",
        "    print(f\"{entity}: {count}\")\n",
        "\n",
        "# Display a few examples of the NER output\n",
        "print(\"\\nSample NER Output:\")\n",
        "print(df[['lemmatized_text', 'entities']].head())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Comment**\n",
        "Make sure to submit the cleaned data CSV in the comment section - 10 points"
      ],
      "metadata": {
        "id": "CXNn1lEVbMsv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# This will download the CSV file to your local machine\n",
        "files.download('keyboard_reviews_cleaned.csv')\n"
      ],
      "metadata": {
        "id": "qYRO5Cn8bYwZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "206cf8e2-e072-41c2-ecc8-5acd4ed60d9e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_56ccf3db-9082-4ea7-865b-904bc574b359\", \"keyboard_reviews_cleaned.csv\", 311680)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ],
      "metadata": {
        "id": "q8BFCvWp32cf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "The assignment was great. It was a eye-opener to the world of web scraping. I look forward to more such assignments. The only issue noted was that some\n",
        "sites did not respond consistently to scraping. Some were even blocking. This required me to make a lot of changes in the code and the downside is that it brought alot of\n",
        "error which were not my own making. I feel the assignment is hard to complete in the provided time period.\n",
        "'''"
      ],
      "metadata": {
        "id": "_e557s2w4BpK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "f3d3ab61-98d6-4722-934e-7621ce21b7ec"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nThe assignment was great. It was a eye-opener to the world of web scraping. I look forward to more such assignments. The only issue noted was that some\\nsites did not respond consistently to scraping. Some were even blocking. This required me to make a lot of changes in the code and the downside is that it brought alot of\\nerror which were not my own making. I feel the assignment is hard to complete in the provided time period.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}