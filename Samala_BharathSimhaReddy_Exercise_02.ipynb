{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bharathreddy-2802/BharathSimhaReddy_INFO5731_Fall2024/blob/main/Samala_BharathSimhaReddy_Exercise_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DymRJbxDBCnf"
      },
      "source": [
        "# **INFO5731 In-class Exercise 2**\n",
        "\n",
        "The purpose of this exercise is to understand users' information needs, and then collect data from different sources for analysis by implementing web scraping using Python.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? Specify the amount of data needed for analysis. Provide detailed steps for collecting and saving the data."
      ],
      "metadata": {
        "id": "FBKvD6O_TY6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your answer here\n",
        "'''\n",
        "Research Question:\n",
        "What are the common qualities celebrities have across different industries like acting, music, athletics? More specifically, there schooling, upbringing, awards influence their career?\n",
        "Data to Collect:\n",
        "Name\n",
        "Industry (actor, musician, athlete, leader)\n",
        "Early life (including birthdate, place, and family background)\n",
        "Education level\n",
        "Career start (first major role or achievement)\n",
        "Notable awards or recognitions\n",
        "Social media influence (followers or media mentions)\n",
        "\n",
        "Amount of Data:\n",
        "Collect data for 1,000 celebrity profiles.\n",
        "These samples should span different industries to ensure diversity.\n",
        "\n",
        "Steps for Collecting Data:\n",
        "\n",
        "Identify Celebrity Categories:\n",
        "Predefine a list of industries (actors, musicians, athletes) to categorize the celebrities.\n",
        "Select Wikipedia as the Source.\n",
        "Web Scraping Process:\n",
        "Use Python with BeautifulSoup to scrape the celebrity profiles.\n",
        "Ensure the profiles contain all required fields (education, awards).\n",
        "Handle missing data with placeholder values.\n",
        "Save the collected data into a CSV or JSON file for further analysis.\n",
        "'''\n"
      ],
      "metadata": {
        "id": "cikVKDXdTbzE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "0a826ac2-4d64-4106-93d5-efc972316463"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nResearch Question:\\nWhat are the common qualities celebrities have across different industries like acting, music, athletics? More specifically, there schooling, upbringing, awards influence their career?\\nData to Collect:\\nName\\nIndustry (actor, musician, athlete, leader)\\nEarly life (including birthdate, place, and family background)\\nEducation level\\nCareer start (first major role or achievement)\\nNotable awards or recognitions\\nSocial media influence (followers or media mentions)\\n\\nAmount of Data:\\nCollect data for 1,000 celebrity profiles.\\nThese samples should span different industries to ensure diversity.\\n\\nSteps for Collecting Data:\\n\\nIdentify Celebrity Categories:\\nPredefine a list of industries (actors, musicians, athletes) to categorize the celebrities.\\nSelect Wikipedia as the Source.\\nWeb Scraping Process:\\nUse Python with BeautifulSoup to scrape the celebrity profiles.\\nEnsure the profiles contain all required fields (education, awards).\\nHandle missing data with placeholder values.\\nSave the collected data into a CSV or JSON file for further analysis.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 (10 Points)\n",
        "Write Python code to collect a dataset of 1000 samples related to the question discussed in Question 1."
      ],
      "metadata": {
        "id": "E9RqrlwdTfvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import urllib.parse\n",
        "\n",
        "base_url = 'https://en.wikipedia.org/wiki/Category:'\n",
        "\n",
        "categories = [\n",
        "    'American_actors',\n",
        "    'British_musicians',\n",
        "    'Athletes',\n",
        "    'Current_heads_of_state',\n",
        "    'Former_heads_of_state',\n",
        "    'World_leaders'\n",
        "]\n",
        "\n",
        "# Function to scrape a category page\n",
        "def scrape_category(category):\n",
        "    url = base_url + category\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Extract celebrity links\n",
        "    celeb_links = []\n",
        "    for link in soup.find_all('a', href=True):\n",
        "        href = link['href']\n",
        "        if href.startswith('/wiki/') and ':' not in href:\n",
        "            full_url = urllib.parse.urljoin('https://en.wikipedia.org', href)\n",
        "            celeb_links.append(full_url)\n",
        "\n",
        "    return celeb_links\n",
        "\n",
        "# Function to scrape celebrity profile\n",
        "def scrape_celebrity_profile(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        name_tag = soup.find('h1', {'id': 'firstHeading'})\n",
        "        name = name_tag.text if name_tag else 'Unknown'\n",
        "\n",
        "        infobox = soup.find('table', {'class': 'infobox'})\n",
        "        details = {}\n",
        "\n",
        "        if infobox:\n",
        "            for tr in infobox.find_all('tr'):\n",
        "                th = tr.find('th')\n",
        "                td = tr.find('td')\n",
        "                if th and td:\n",
        "                    key = th.text.strip()\n",
        "                    value = td.text.strip()\n",
        "                    details[key] = value\n",
        "\n",
        "        return {\n",
        "            'Name': name,\n",
        "            'Details': details\n",
        "        }\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Request failed for {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "celebrity_data = []\n",
        "\n",
        "for category in categories:\n",
        "    celeb_links = scrape_category(category)\n",
        "    for link in celeb_links:\n",
        "        profile = scrape_celebrity_profile(link)\n",
        "        if profile:\n",
        "            celebrity_data.append(profile)\n",
        "        if len(celebrity_data) >= 1000:\n",
        "            break\n",
        "\n",
        "df = pd.DataFrame(celebrity_data)\n",
        "\n",
        "df.to_csv('celebrity_profiles.csv', index=False)\n",
        "\n",
        "print(\"Data collection complete. Saved to 'celebrity_profiles.csv'\")\n"
      ],
      "metadata": {
        "id": "4XvRknixTh1g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "beba1523-d03a-4498-c0d9-bc211873ab42"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data collection complete. Saved to 'celebrity_profiles.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jb4GZsBkBS"
      },
      "source": [
        "## Question 3 (10 Points)\n",
        "Write Python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"XYZ\". The articles should be published in the last 10 years (2014-2024).\n",
        "\n",
        "The following information from the article needs to be collected:\n",
        "\n",
        "(1) Title of the article\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# write your answer here\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "def scholar_data(keyword, num_papers, start_year, end_year):\n",
        "    base_url = \"https://scholar.google.com/scholar\"\n",
        "    collected_data = []\n",
        "    params = {\n",
        "        'q': keyword,\n",
        "        'hl': 'en',\n",
        "        'as_ylo': start_year,\n",
        "        'as_yhi': end_year\n",
        "    }\n",
        "\n",
        "\n",
        "    # Loop to move through results\n",
        "    for start in range(0, num_papers, 10):\n",
        "        params['start'] = start\n",
        "        response = requests.get(base_url, params=params)\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Failed to retrieve data: Status code {response.status_code}\")\n",
        "            break\n",
        "\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Find all result containers\n",
        "        results = soup.find_all('div', class_='gs_ri')\n",
        "        if not results:\n",
        "            print(\"No more results found in parsing\")\n",
        "            break\n",
        "\n",
        "        for result in results:\n",
        "            title_elem = result.find('h3', class_='gs_rt')\n",
        "            title = title_elem.text if title_elem else 'N/A'\n",
        "\n",
        "            #Journel\n",
        "            venue_elem = result.fin('div', class_='gs_a')\n",
        "            venue = venue_elem.text if venue_elem else 'N/A'\n",
        "\n",
        "            #Abstract\n",
        "            abstract_elem = result.find('div', class_='gs_rs')\n",
        "            abstract = abstract_elem.text if abstract_elem else 'N/A'\n",
        "            #Year\n",
        "            year = 'N/A'\n",
        "            for text in venue.split():\n",
        "                if text.isdigit() and len(text) == 4 and start_year <= int(text) <= end_year:\n",
        "                    year = text\n",
        "                    break\n",
        "\n",
        "            #Authors\n",
        "            authors = venue.split('-')[0].strip()\n",
        "\n",
        "            collected_data.append({\n",
        "                'Title': title,\n",
        "                'Venue': venue,\n",
        "                'Year': year,\n",
        "                'Authors': authors,\n",
        "                'Abstract': abstract\n",
        "            })\n",
        "        #Progress reporting\n",
        "        print(f\"Retrieved {len(collected_data)}/{num_papers} papers.\")\n",
        "\n",
        "        #Delay to avoid hitting limits\n",
        "        time.sleep(10)\n",
        "    return collected_data\n",
        "\n",
        "papers = scholar_data(keyword=\"XYZ\", num_papers=1000, start_year=2014, end_year=2024)\n",
        "\n",
        "#check if any data was collected before saving\n",
        "if papers:\n",
        "  df = pd.DataFrame(papers)\n",
        "  df.to_csv('google_scholar_data.csv', index=False)\n",
        "  print(\"Data collection completed. Saved to google_scholar_data.csv\")\n",
        "else:\n",
        "  print(\"No data collected.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBNyl70tOF-e",
        "outputId": "1608a764-1dd9-4ebf-c560-3ec59e7b46eb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to retrieve data: Status code 429\n",
            "No data collected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJDe71iLB616"
      },
      "source": [
        "## Question 4A (10 Points)\n",
        "Develop Python code to collect data from social media platforms like Reddit, Instagram, Twitter (formerly known as X), Facebook, or any other. Use hashtags, keywords, usernames, or user IDs to gather the data.\n",
        "\n",
        "\n",
        "\n",
        "Ensure that the collected data has more than four columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtKskTzbCLaU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f4f7d94-c463-41c9-c7e9-ff500790d042"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting praw\n",
            "  Downloading praw-7.7.1-py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting prawcore<3,>=2.1 (from praw)\n",
            "  Downloading prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting update-checker>=0.18 (from praw)\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.10/dist-packages (from praw) (1.8.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from prawcore<3,>=2.1->praw) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2024.8.30)\n",
            "Downloading praw-7.7.1-py3-none-any.whl (191 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.0/191.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
            "Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Installing collected packages: update-checker, prawcore, praw\n",
            "Successfully installed praw-7.7.1 prawcore-2.4.0 update-checker-0.18.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data has been saved to 'reddit_data.csv'.\n"
          ]
        }
      ],
      "source": [
        "!pip install praw\n",
        "import praw\n",
        "import pandas as pd\n",
        "\n",
        "client_id = 'ACvTzL0VncEYxw8FeI5DvA'\n",
        "client_secret = 'xTEqk4AG8Juk4faRYSYVmPLPsWnrhw'\n",
        "user_agent = 'Collect Data'\n",
        "\n",
        "reddit = praw.Reddit(\n",
        "    client_id=client_id,\n",
        "    client_secret=client_secret,\n",
        "    user_agent=user_agent\n",
        ")\n",
        "\n",
        "search_keyword = 'Machine Learning'\n",
        "subreddit = 'all'\n",
        "\n",
        "def collect_reddit_data(keyword, subreddit_name, limit=100):\n",
        "    posts_data = []\n",
        "    subreddit = reddit.subreddit(subreddit_name)\n",
        "\n",
        "    for submission in subreddit.search(keyword, limit=limit):\n",
        "        posts_data.append({\n",
        "            'Title': submission.title,\n",
        "            'Author': str(submission.author),\n",
        "            'Score': submission.score,\n",
        "            'URL': submission.url,\n",
        "            'Created_UTC': submission.created_utc,\n",
        "            'Text': submission.selftext\n",
        "        })\n",
        "\n",
        "    return posts_data\n",
        "\n",
        "posts_data = collect_reddit_data(search_keyword, subreddit)\n",
        "\n",
        "df = pd.DataFrame(posts_data)\n",
        "\n",
        "df.to_csv('reddit_data.csv', index=False)\n",
        "\n",
        "print(f\"Data has been saved to 'reddit_data.csv'.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55W9AMdXCSpV"
      },
      "source": [
        "## Question 4B (10 Points)\n",
        "If you encounter challenges with Question-4 web scraping using Python, employ any online tools such as ParseHub or Octoparse for data extraction. Introduce the selected tool, outline the steps for web scraping, and showcase the final output in formats like CSV or Excel.\n",
        "\n",
        "\n",
        "\n",
        "Upload a document (Word or PDF File) in any shared storage (preferably UNT OneDrive) and add the publicly accessible link in the below code cell.\n",
        "\n",
        "Please only choose one option for question 4. If you do both options, we will grade only the first one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I57NXsauCec2"
      },
      "outputs": [],
      "source": [
        "# write your answer here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "sZOhks1dXWEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on Web Scraping and Data Collection**\n",
        "\n",
        "\n",
        "\n",
        "Please share your thoughts and feedback on the web scraping and data collection exercises you have completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on web scraping tasks. What were the key concepts or techniques you found most beneficial in understanding the process of extracting data from various online sources?\n",
        "\n",
        "\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them? If you opted for the non-coding option, share your experience with the chosen tool.\n",
        "\n",
        "\n",
        "\n",
        "Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work or research?\n",
        "\n",
        "**(no grading of your submission if this question is left unanswered)**"
      ],
      "metadata": {
        "id": "eqmHVEwaWhbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "I felt the in class activity was hard and needed much more time to complete it.\n",
        "'''"
      ],
      "metadata": {
        "id": "akAVJn9YBTQT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "04ab5537-3897-482a-fb6c-4942e7dab3ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nI felt the in class activity was hard and needed much more time to complete it.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "55W9AMdXCSpV",
        "sZOhks1dXWEe"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}